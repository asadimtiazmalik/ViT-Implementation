{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing packages**"
      ],
      "metadata": {
        "id": "CXgIva8Pn-aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "from tqdm.notebook import tqdm\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.transforms import RandomCrop, ToTensor, Normalize, Compose, Resize, RandomHorizontalFlip"
      ],
      "metadata": {
        "id": "GCClIJxjnoLd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Setting Hyperparameters for the Implementation**"
      ],
      "metadata": {
        "id": "4Etqkj7gw7QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# Hyperparameters used in the paper \"Vision Transformers\"\n",
        "num_of_heads = 12\n",
        "num_of_encoders = 12\n",
        "patch_size = 16\n",
        "latent_vector_size = 768\n",
        "dropout = 0.1\n",
        "num_channels = 3\n",
        "\n",
        "# For training we will use CIFAR-10\n",
        "num_classes = 10\n",
        "size = 224\n",
        "\n",
        "# Training parameters\n",
        "epochs = 10\n",
        "lr = 10e-3\n",
        "weight_decay = 0.03\n",
        "batch_size = 2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfMAjqWeoQMy",
        "outputId": "279dcd22-4709-4237-ae83-013474106d23"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Linearly embedding image patches with positional encoding vector and generating a linearly projected version of the input image**"
      ],
      "metadata": {
        "id": "fxFh-Joj8tiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, patch_size = patch_size, num_channels = num_channels, latent_vector_size = latent_vector_size, device = device, batch_size = batch_size):\n",
        "    super(InputEmbedding, self).__init__()\n",
        "    self.latent_vector_size = latent_vector_size\n",
        "    self.patch_size = patch_size\n",
        "    self.num_channels = num_channels\n",
        "    self.device = device\n",
        "    self.batch_size = batch_size\n",
        "    self.input_size = self.patch_size**2 * self.num_channels\n",
        "\n",
        "    # Linear projection layer to prepend the class tokens and append the positional encodings to the patch embeddings\n",
        "    self.liner_proj = nn.Linear(in_features = self.input_size, out_features= self.latent_vector_size)\n",
        "    # Hybrid model for generating projection from Conv generated feature map\n",
        "    # self.conv_proj = nn.Conv2d(in_channels = self.input_size, out_channels= self.latent_vector_size, kernel_size=self.patch_size)\n",
        "    # Similar to BERTâ€™s [class] token, we prepend a learnable 1D embedding (hence using nn.Parameter()) to the sequence of embedded patches\n",
        "    self.class_embeddings = nn.Parameter(torch.randn([self.batch_size, 1, self.input_size]))\n",
        "    # We use standard learnable 1D position embeddings\n",
        "    self.position_embeddings = nn.Parameter(torch.randn([self.batch_size, 1, self.input_size]))\n",
        "\n",
        "  def forward(self, input_image):\n",
        "      # N (Num Patches) = HW // P^(2)\n",
        "      # We reshape the input image from B,C,H,W to B,N,P^(2)*C\n",
        "      batch_size, _,  height, width = input_image.shape\n",
        "      input_image = input_image.to(self.device)\n",
        "      num_patches = (height * width)//(self.patch_size**2)\n",
        "      patches = input_image.view((batch_size, num_patches, self.input_size))\n",
        "\n",
        "      # Generate linear projections\n",
        "      linear_proj = self.liner_proj(patches)\n",
        "      # Prepend class tokens\n",
        "      patch_embeddings = torch.cat((linear_proj,self.class_embeddings), axis=1)\n",
        "      # Add positional embeddings to the patch embeddings\n",
        "      patch_embeddings += self.position_embeddings\n",
        "\n",
        "      return patch_embeddings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A6lHflvMxNZ8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implement Encoder for the Vision transformer**"
      ],
      "metadata": {
        "id": "uRKWDGT5oaUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_of_heads=num_of_heads, num_of_encoder=num_of_encoders, latent_vector_size = latent_vector_size, device = device, dropout=dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.num_of_heads = num_of_heads\n",
        "    self.num_of_encoders = num_of_encoders\n",
        "    self.latent_vector_size = latent_vector_size\n",
        "    self.device = device\n",
        "    self.dropout = dropout\n",
        "\n",
        "    # Layer Norm\n",
        "    self.norm = nn.LayerNorm(self.latent_vector_size)\n",
        "    # Multi-Headed Attention\n",
        "    self.multiheadedattn = nn.MultiheadAttention(embed_dim=self.latent_vector_size, num_heads=self.num_of_heads, dropout=self.dropout)\n",
        "    # MLP with GELU\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(self.latent_vector_size, self.latent_vector_size*4),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(self.dropout),\n",
        "        nn.Linear(self.latent_vector_size*4, self.latent_vector_size),\n",
        "        nn.Dropout(self.dropout)\n",
        "        )\n",
        "  def forward(self, patch_embeddings):\n",
        "    # apply layer norm n\n",
        "    first_norm = self.norm(patch_embeddings)\n",
        "    # attn output is stored in the first dimension\n",
        "    multiheadedattn = self.multiheadedattn(first_norm,first_norm,first_norm)[0]\n",
        "    # add the skip connection between attn output and patch embeddings\n",
        "    skip_conn_1 = patch_embeddings + multiheadedattn\n",
        "    # apply layer n + 1\n",
        "    second_norm = self.norm(skip_conn_1)\n",
        "    # feed norm output to the MLP\n",
        "    mlp_output = self.mlp(second_norm)\n",
        "    # second attn layer output\n",
        "    output = skip_conn_1 + multiheadedattn\n",
        "\n",
        "    # This output can be fed over and over into the encoder block for this implementation we will assume\n",
        "    # we have only one MULTI_HEADED_ATTN + MLP block\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "JjTh-H5-oZnX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Vision Transformer class with Input Embedding and Encoder block**"
      ],
      "metadata": {
        "id": "99-ncdGujAw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, num_of_encoders=num_of_encoders, device=device, num_classes=num_classes, latent_vector_size = latent_vector_size, dropout=dropout):\n",
        "    super(ViT, self).__init__()\n",
        "    self.num_of_encoders=num_of_encoders\n",
        "    self.device=device\n",
        "    self.num_classes=num_classes\n",
        "    self.latent_vector_size = latent_vector_size\n",
        "    self.dropout=dropout\n",
        "\n",
        "    self.input_emb = InputEmbedding()\n",
        "    self.encoders = nn.ModuleList([Encoder() for i in range(self.num_of_encoders)])\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.LayerNorm(self.latent_vector_size),\n",
        "        nn.Linear(self.latent_vector_size,self.latent_vector_size),\n",
        "        nn.Linear(self.latent_vector_size,self.latent_vector_size),\n",
        "        nn.Linear(self.latent_vector_size,self.latent_vector_size),\n",
        "        nn.Linear(self.latent_vector_size,self.latent_vector_size),\n",
        "        nn.Linear(self.latent_vector_size,self.num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, image):\n",
        "    patch_emb = self.input_emb(image)\n",
        "    for encoder in self.encoders:\n",
        "      patch_emb = encoder(patch_emb)\n",
        "\n",
        "    # Take the first token from the final embeddings from the encoder block\n",
        "    cls_token = patch_emb[:,0]\n",
        "    # Final pass through the MLP Head\n",
        "    cls_output = self.mlp(cls_token)\n",
        "\n",
        "    return cls_output\n"
      ],
      "metadata": {
        "id": "N5_Q2ryoIf5I"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Testing the developed network**"
      ],
      "metadata": {
        "id": "NKE8sH9bjI-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your image tensor\n",
        "x = torch.randn((2,3,224,224))\n",
        "# Initialize Vision Transformer\n",
        "y = ViT().to(device)\n",
        "# Feed the image tensor into the network\n",
        "y(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBrGuUWdQzN6",
        "outputId": "63036dce-a13d-4663-b814-127720462c32"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0910,  0.0151,  0.1366, -0.0526, -0.0780,  0.1485, -0.0569,  0.1377,\n",
              "         -0.0394,  0.0993],\n",
              "        [ 0.0837,  0.0483,  0.0860,  0.0017, -0.1048,  0.0643,  0.0077,  0.1063,\n",
              "         -0.0646,  0.0935]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Visualizing the Computational Graph**"
      ],
      "metadata": {
        "id": "NC8jNNDijNqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(y, (3,224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPUqj5nh3Cjk",
        "outputId": "87f2f38b-10e8-43f7-fa1e-eed6ce55bfc5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1             [-1, 196, 768]         590,592\n",
            "    InputEmbedding-2             [-1, 197, 768]               0\n",
            "         LayerNorm-3             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-4  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "         LayerNorm-5             [-1, 197, 768]           1,536\n",
            "            Linear-6            [-1, 197, 3072]       2,362,368\n",
            "              GELU-7            [-1, 197, 3072]               0\n",
            "           Dropout-8            [-1, 197, 3072]               0\n",
            "            Linear-9             [-1, 197, 768]       2,360,064\n",
            "          Dropout-10             [-1, 197, 768]               0\n",
            "          Encoder-11             [-1, 197, 768]               0\n",
            "        LayerNorm-12             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-13  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-14             [-1, 197, 768]           1,536\n",
            "           Linear-15            [-1, 197, 3072]       2,362,368\n",
            "             GELU-16            [-1, 197, 3072]               0\n",
            "          Dropout-17            [-1, 197, 3072]               0\n",
            "           Linear-18             [-1, 197, 768]       2,360,064\n",
            "          Dropout-19             [-1, 197, 768]               0\n",
            "          Encoder-20             [-1, 197, 768]               0\n",
            "        LayerNorm-21             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-22  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-23             [-1, 197, 768]           1,536\n",
            "           Linear-24            [-1, 197, 3072]       2,362,368\n",
            "             GELU-25            [-1, 197, 3072]               0\n",
            "          Dropout-26            [-1, 197, 3072]               0\n",
            "           Linear-27             [-1, 197, 768]       2,360,064\n",
            "          Dropout-28             [-1, 197, 768]               0\n",
            "          Encoder-29             [-1, 197, 768]               0\n",
            "        LayerNorm-30             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-31  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-32             [-1, 197, 768]           1,536\n",
            "           Linear-33            [-1, 197, 3072]       2,362,368\n",
            "             GELU-34            [-1, 197, 3072]               0\n",
            "          Dropout-35            [-1, 197, 3072]               0\n",
            "           Linear-36             [-1, 197, 768]       2,360,064\n",
            "          Dropout-37             [-1, 197, 768]               0\n",
            "          Encoder-38             [-1, 197, 768]               0\n",
            "        LayerNorm-39             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-40  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-41             [-1, 197, 768]           1,536\n",
            "           Linear-42            [-1, 197, 3072]       2,362,368\n",
            "             GELU-43            [-1, 197, 3072]               0\n",
            "          Dropout-44            [-1, 197, 3072]               0\n",
            "           Linear-45             [-1, 197, 768]       2,360,064\n",
            "          Dropout-46             [-1, 197, 768]               0\n",
            "          Encoder-47             [-1, 197, 768]               0\n",
            "        LayerNorm-48             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-49  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-50             [-1, 197, 768]           1,536\n",
            "           Linear-51            [-1, 197, 3072]       2,362,368\n",
            "             GELU-52            [-1, 197, 3072]               0\n",
            "          Dropout-53            [-1, 197, 3072]               0\n",
            "           Linear-54             [-1, 197, 768]       2,360,064\n",
            "          Dropout-55             [-1, 197, 768]               0\n",
            "          Encoder-56             [-1, 197, 768]               0\n",
            "        LayerNorm-57             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-58  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-59             [-1, 197, 768]           1,536\n",
            "           Linear-60            [-1, 197, 3072]       2,362,368\n",
            "             GELU-61            [-1, 197, 3072]               0\n",
            "          Dropout-62            [-1, 197, 3072]               0\n",
            "           Linear-63             [-1, 197, 768]       2,360,064\n",
            "          Dropout-64             [-1, 197, 768]               0\n",
            "          Encoder-65             [-1, 197, 768]               0\n",
            "        LayerNorm-66             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-67  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-68             [-1, 197, 768]           1,536\n",
            "           Linear-69            [-1, 197, 3072]       2,362,368\n",
            "             GELU-70            [-1, 197, 3072]               0\n",
            "          Dropout-71            [-1, 197, 3072]               0\n",
            "           Linear-72             [-1, 197, 768]       2,360,064\n",
            "          Dropout-73             [-1, 197, 768]               0\n",
            "          Encoder-74             [-1, 197, 768]               0\n",
            "        LayerNorm-75             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-76  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-77             [-1, 197, 768]           1,536\n",
            "           Linear-78            [-1, 197, 3072]       2,362,368\n",
            "             GELU-79            [-1, 197, 3072]               0\n",
            "          Dropout-80            [-1, 197, 3072]               0\n",
            "           Linear-81             [-1, 197, 768]       2,360,064\n",
            "          Dropout-82             [-1, 197, 768]               0\n",
            "          Encoder-83             [-1, 197, 768]               0\n",
            "        LayerNorm-84             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-85  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-86             [-1, 197, 768]           1,536\n",
            "           Linear-87            [-1, 197, 3072]       2,362,368\n",
            "             GELU-88            [-1, 197, 3072]               0\n",
            "          Dropout-89            [-1, 197, 3072]               0\n",
            "           Linear-90             [-1, 197, 768]       2,360,064\n",
            "          Dropout-91             [-1, 197, 768]               0\n",
            "          Encoder-92             [-1, 197, 768]               0\n",
            "        LayerNorm-93             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-94  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "        LayerNorm-95             [-1, 197, 768]           1,536\n",
            "           Linear-96            [-1, 197, 3072]       2,362,368\n",
            "             GELU-97            [-1, 197, 3072]               0\n",
            "          Dropout-98            [-1, 197, 3072]               0\n",
            "           Linear-99             [-1, 197, 768]       2,360,064\n",
            "         Dropout-100             [-1, 197, 768]               0\n",
            "         Encoder-101             [-1, 197, 768]               0\n",
            "       LayerNorm-102             [-1, 197, 768]           1,536\n",
            "MultiheadAttention-103  [[-1, 197, 768], [-1, 2, 2]]               0\n",
            "       LayerNorm-104             [-1, 197, 768]           1,536\n",
            "          Linear-105            [-1, 197, 3072]       2,362,368\n",
            "            GELU-106            [-1, 197, 3072]               0\n",
            "         Dropout-107            [-1, 197, 3072]               0\n",
            "          Linear-108             [-1, 197, 768]       2,360,064\n",
            "         Dropout-109             [-1, 197, 768]               0\n",
            "         Encoder-110             [-1, 197, 768]               0\n",
            "       LayerNorm-111                  [-1, 768]           1,536\n",
            "          Linear-112                  [-1, 768]         590,592\n",
            "          Linear-113                  [-1, 768]         590,592\n",
            "          Linear-114                  [-1, 768]         590,592\n",
            "          Linear-115                  [-1, 768]         590,592\n",
            "          Linear-116                   [-1, 10]           7,690\n",
            "================================================================\n",
            "Total params: 59,668,234\n",
            "Trainable params: 59,668,234\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 182.40\n",
            "Params size (MB): 227.62\n",
            "Estimated Total Size (MB): 410.59\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}